{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce904a8",
   "metadata": {},
   "source": [
    "# ğŸ§  Neural Storyteller â€“ Image Captioning with Seq2Seq\n",
    "\n",
    "**Objective:** Build a multimodal deep learning model that generates natural language descriptions for images using a Sequence-to-Sequence (Seq2Seq) architecture.\n",
    "\n",
    "**Architecture:**\n",
    "- **Encoder:** Pre-trained ResNet50 â†’ Linear projection (2048 â†’ 512)\n",
    "- **Decoder:** LSTM with word embeddings, outputs vocabulary distribution\n",
    "\n",
    "**Dataset:** Flickr30k (31,783 images with 5 captions each)\n",
    "\n",
    "**Deployment:** All artifacts saved to `/content/deploy/` for Gradio Hugging Face Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145f609",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Cell 1: Install Dependencies & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision nltk gradio Pillow matplotlib tqdm kagglehub numpy pandas\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score as nltk_meteor\n",
    "\n",
    "# â”€â”€ Reproducibility â”€â”€\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# â”€â”€ Device â”€â”€\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# â”€â”€ Paths (Colab /content/) â”€â”€\n",
    "BASE_DIR       = \"/content\"\n",
    "FEATURES_FILE  = os.path.join(BASE_DIR, \"flickr30k_features.pkl\")\n",
    "VOCAB_FILE     = os.path.join(BASE_DIR, \"vocab.pkl\")\n",
    "MODEL_FILE     = os.path.join(BASE_DIR, \"best_model.pth\")\n",
    "DEPLOY_DIR     = os.path.join(BASE_DIR, \"deploy\")\n",
    "os.makedirs(DEPLOY_DIR, exist_ok=True)\n",
    "print(\"âœ… All dependencies installed and paths configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8362c8",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Cell 2: Download & Prepare Flickr30k Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd74e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download Flickr30k from Kaggle  (authenticates via KAGGLE_USERNAME / KAGGLE_KEY)\n",
    "dataset_path = kagglehub.dataset_download(\"adityajn105/flickr30k\")\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "\n",
    "# â”€â”€ Locate images folder & captions file â”€â”€\n",
    "IMAGE_DIR    = None\n",
    "CAPTIONS_FILE = None\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    jpg_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg'))]\n",
    "    if len(jpg_files) > 1000:\n",
    "        IMAGE_DIR = root\n",
    "    if 'captions.txt' in files:\n",
    "        CAPTIONS_FILE = os.path.join(root, 'captions.txt')\n",
    "\n",
    "# Fallback: also check for results.csv / captions.csv\n",
    "if CAPTIONS_FILE is None:\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for f in files:\n",
    "            if f.endswith('.csv') and 'caption' in f.lower():\n",
    "                CAPTIONS_FILE = os.path.join(root, f)\n",
    "                break\n",
    "            if f == 'results.csv':\n",
    "                CAPTIONS_FILE = os.path.join(root, f)\n",
    "                break\n",
    "\n",
    "assert IMAGE_DIR is not None,    \"âŒ Could not locate images folder!\"\n",
    "assert CAPTIONS_FILE is not None, \"âŒ Could not locate captions file!\"\n",
    "\n",
    "num_images = len([f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(('.jpg', '.jpeg'))])\n",
    "print(f\"âœ… Found {num_images} images at: {IMAGE_DIR}\")\n",
    "print(f\"âœ… Captions file at:  {CAPTIONS_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2699001e",
   "metadata": {},
   "source": [
    "## ğŸ–¼ï¸ Part 1: Feature Extraction Pipeline with ResNet50\n",
    "\n",
    "Extract a **2048-dimensional** feature vector from every image using a pre-trained ResNet50 (final classification layer removed). Features are cached to `flickr30k_features.pkl` so we only need to run this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b27fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Part 1 â€“ Feature Extraction Pipeline  (run ONCE, then skip)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class FlickrImageDataset(Dataset):\n",
    "    \"\"\"Loads every .jpg from IMAGE_DIR, applies transform.\"\"\"\n",
    "    def __init__(self, img_dir, transform):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_names = sorted(\n",
    "            [f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.jpeg'))]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.img_names[idx]\n",
    "        img = Image.open(os.path.join(self.img_dir, name)).convert('RGB')\n",
    "        return self.transform(img), name\n",
    "\n",
    "# â”€â”€ ResNet-50 feature extractor (remove final FC) â”€â”€\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-1])          # output: (B, 2048, 1, 1)\n",
    "resnet = nn.DataParallel(resnet).to(device)\n",
    "resnet.eval()\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "img_dataset = FlickrImageDataset(IMAGE_DIR, img_transform)\n",
    "img_loader  = DataLoader(img_dataset, batch_size=128, num_workers=4, pin_memory=True)\n",
    "\n",
    "features_dict = {}\n",
    "with torch.no_grad():\n",
    "    for imgs, names in tqdm(img_loader, desc=\"Extracting ResNet50 Features\"):\n",
    "        feats = resnet(imgs.to(device)).view(imgs.size(0), -1)   # (B, 2048)\n",
    "        for i, name in enumerate(names):\n",
    "            features_dict[name] = feats[i].cpu().numpy()\n",
    "\n",
    "with open(FEATURES_FILE, 'wb') as f:\n",
    "    pickle.dump(features_dict, f)\n",
    "\n",
    "print(f\"\\nâœ… Success! {len(features_dict)} images â†’ 2048-d vectors saved to {FEATURES_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd62cb5",
   "metadata": {},
   "source": [
    "## ğŸ“ Part 2: Load Captions & Build Vocabulary\n",
    "\n",
    "Load `captions.txt`, clean the text, and build a word-to-index vocabulary with special tokens `<pad>`, `<start>`, `<end>`, `<unk>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083dfce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Part 2 â€“ Vocabulary & Text Pre-Processing\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# â”€â”€ Load captions â”€â”€\n",
    "captions_df = pd.read_csv(CAPTIONS_FILE, sep='|' if '|' in open(CAPTIONS_FILE).readline() else ',')\n",
    "# Normalise column names\n",
    "captions_df.columns = [c.strip().lower().replace(' ', '_') for c in captions_df.columns]\n",
    "# Rename to standard names\n",
    "rename_map = {}\n",
    "for c in captions_df.columns:\n",
    "    if 'image' in c:\n",
    "        rename_map[c] = 'image'\n",
    "    elif 'caption' in c:\n",
    "        rename_map[c] = 'caption'\n",
    "captions_df.rename(columns=rename_map, inplace=True)\n",
    "captions_df.dropna(subset=['image', 'caption'], inplace=True)\n",
    "captions_df['image'] = captions_df['image'].str.strip()\n",
    "captions_df['caption'] = captions_df['caption'].str.strip()\n",
    "print(f\"Loaded {len(captions_df)} caption rows for {captions_df['image'].nunique()} images\")\n",
    "\n",
    "# â”€â”€ Clean text â”€â”€\n",
    "def clean_caption(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)                       # keep letters & spaces\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if len(t) > 1 or t in ('a', 'i')]  # drop single chars (except a/i)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "captions_df['caption_clean'] = captions_df['caption'].apply(clean_caption)\n",
    "print(\"Sample cleaned captions:\")\n",
    "print(captions_df[['image', 'caption_clean']].head(10).to_string(index=False))\n",
    "\n",
    "# â”€â”€ Vocabulary class â”€â”€\n",
    "class Vocabulary:\n",
    "    PAD, START, END, UNK = '<pad>', '<start>', '<end>', '<unk>'\n",
    "\n",
    "    def __init__(self, freq_threshold=5):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self._idx = 0\n",
    "        # Add special tokens first\n",
    "        for tok in [self.PAD, self.START, self.END, self.UNK]:\n",
    "            self._add_word(tok)\n",
    "\n",
    "    def _add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self._idx\n",
    "            self.idx2word[self._idx] = word\n",
    "            self._idx += 1\n",
    "\n",
    "    def build(self, sentences):\n",
    "        counter = Counter()\n",
    "        for sent in sentences:\n",
    "            counter.update(sent.split())\n",
    "        for word, cnt in counter.items():\n",
    "            if cnt >= self.freq_threshold:\n",
    "                self._add_word(word)\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        return [self.word2idx.get(w, self.word2idx[self.UNK]) for w in text.split()]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        words = []\n",
    "        for idx in indices:\n",
    "            w = self.idx2word.get(idx, self.UNK)\n",
    "            if w == self.END:\n",
    "                break\n",
    "            if w not in (self.PAD, self.START):\n",
    "                words.append(w)\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "vocab = Vocabulary(freq_threshold=5)\n",
    "vocab.build(captions_df['caption_clean'].tolist())\n",
    "print(f\"\\nâœ… Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Save vocab\n",
    "with open(VOCAB_FILE, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(f\"Vocabulary saved to {VOCAB_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84edb57c",
   "metadata": {},
   "source": [
    "## ğŸ—‚ï¸ Part 3: Dataset Class & Train / Val / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25506fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Part 3 â€“ Caption Dataset & Data Loaders\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Load cached features\n",
    "with open(FEATURES_FILE, 'rb') as f:\n",
    "    features_dict = pickle.load(f)\n",
    "print(f\"Loaded {len(features_dict)} image features\")\n",
    "\n",
    "# Keep only images that have both features AND captions\n",
    "available_images = set(features_dict.keys()) & set(captions_df['image'].unique())\n",
    "captions_df = captions_df[captions_df['image'].isin(available_images)].reset_index(drop=True)\n",
    "print(f\"Using {len(available_images)} images with {len(captions_df)} caption rows\")\n",
    "\n",
    "# â”€â”€ Train / Val / Test split (80 / 10 / 10 by image) â”€â”€\n",
    "all_images = sorted(available_images)\n",
    "random.shuffle(all_images)\n",
    "n = len(all_images)\n",
    "train_imgs = set(all_images[:int(0.8 * n)])\n",
    "val_imgs   = set(all_images[int(0.8 * n):int(0.9 * n)])\n",
    "test_imgs  = set(all_images[int(0.9 * n):])\n",
    "print(f\"Split â†’ Train: {len(train_imgs)}  Val: {len(val_imgs)}  Test: {len(test_imgs)}\")\n",
    "\n",
    "MAX_LEN = 40   # max caption length (tokens including <start>/<end>)\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, df, features, vocab, max_len=MAX_LEN):\n",
    "        self.data = df.reset_index(drop=True)\n",
    "        self.features = features\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_name = row['image']\n",
    "        caption  = row['caption_clean']\n",
    "\n",
    "        feat = torch.tensor(self.features[img_name], dtype=torch.float32)\n",
    "\n",
    "        tokens = [self.vocab.word2idx[Vocabulary.START]] + \\\n",
    "                 self.vocab.numericalize(caption) + \\\n",
    "                 [self.vocab.word2idx[Vocabulary.END]]\n",
    "        # Truncate\n",
    "        tokens = tokens[:self.max_len]\n",
    "        length = len(tokens)\n",
    "        # Pad\n",
    "        tokens += [self.vocab.word2idx[Vocabulary.PAD]] * (self.max_len - length)\n",
    "\n",
    "        return feat, torch.tensor(tokens, dtype=torch.long), length\n",
    "\n",
    "def collate_fn(batch):\n",
    "    feats, caps, lengths = zip(*batch)\n",
    "    feats   = torch.stack(feats)\n",
    "    caps    = torch.stack(caps)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    # Sort by descending length for pack_padded_sequence\n",
    "    sorted_idx = lengths.argsort(descending=True)\n",
    "    return feats[sorted_idx], caps[sorted_idx], lengths[sorted_idx]\n",
    "\n",
    "# â”€â”€ Build DataFrames per split â”€â”€\n",
    "train_df = captions_df[captions_df['image'].isin(train_imgs)]\n",
    "val_df   = captions_df[captions_df['image'].isin(val_imgs)]\n",
    "test_df  = captions_df[captions_df['image'].isin(test_imgs)]\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(CaptionDataset(train_df, features_dict, vocab),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(CaptionDataset(val_df, features_dict, vocab),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(CaptionDataset(test_df, features_dict, vocab),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"âœ… DataLoaders ready  |  Train batches: {len(train_loader)}  \"\n",
    "      f\"Val: {len(val_loader)}  Test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9505b6",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Part 4: Seq2Seq Architecture (Encoder + Decoder)\n",
    "\n",
    "| Component | Details |\n",
    "|-----------|---------|\n",
    "| **Encoder** | Linear(2048 â†’ 512) + BatchNorm + ReLU |\n",
    "| **Decoder** | Embedding â†’ LSTM â†’ Linear(hidden â†’ vocab_size) |\n",
    "| **Teacher Forcing** | Used during training |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f8794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Part 4 â€“ Seq2Seq Model\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "EMBED_SIZE  = 256\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS  = 1\n",
    "DROPOUT     = 0.3\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Projects the 2048-d image feature to the LSTM hidden size.\"\"\"\n",
    "    def __init__(self, feature_dim=2048, hidden_size=HIDDEN_SIZE):\n",
    "        super().__init__()\n",
    "        self.fc   = nn.Linear(feature_dim, hidden_size)\n",
    "        self.bn   = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(DROPOUT)\n",
    "\n",
    "    def forward(self, features):\n",
    "        # features: (B, 2048)\n",
    "        x = self.drop(self.relu(self.bn(self.fc(features))))   # (B, hidden)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"LSTM decoder that produces a word at every time-step.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size=EMBED_SIZE,\n",
    "                 hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS):\n",
    "        super().__init__()\n",
    "        self.embed  = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.lstm   = nn.LSTM(embed_size, hidden_size,\n",
    "                              num_layers=num_layers, batch_first=True, dropout=DROPOUT if num_layers > 1 else 0)\n",
    "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.drop   = nn.Dropout(DROPOUT)\n",
    "\n",
    "    def forward(self, captions, hidden, cell):\n",
    "        # captions: (B, T)   hidden/cell: (num_layers, B, hidden)\n",
    "        embeds = self.drop(self.embed(captions))        # (B, T, embed)\n",
    "        output, (hidden, cell) = self.lstm(embeds, (hidden, cell))  # (B, T, hidden)\n",
    "        logits = self.fc_out(output)                    # (B, T, vocab_size)\n",
    "        return logits, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2SeqCaptioner(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=EMBED_SIZE,\n",
    "                 hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS):\n",
    "        super().__init__()\n",
    "        self.encoder    = Encoder(2048, hidden_size)\n",
    "        self.decoder    = Decoder(vocab_size, embed_size, hidden_size, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        features : (B, 2048)\n",
    "        captions : (B, T)   â€“ includes <start> ... <end> <pad>\n",
    "        Returns  : logits (B, T-1, vocab_size)  (predict next word at each step)\n",
    "        \"\"\"\n",
    "        enc_out = self.encoder(features)                         # (B, hidden)\n",
    "        # Repeat for each LSTM layer\n",
    "        h0 = enc_out.unsqueeze(0).repeat(self.num_layers, 1, 1) # (layers, B, hidden)\n",
    "        c0 = torch.zeros_like(h0)\n",
    "\n",
    "        # Teacher forcing: feed captions[:, :-1], predict captions[:, 1:]\n",
    "        logits, _, _ = self.decoder(captions[:, :-1], h0, c0)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# â”€â”€ Instantiate â”€â”€\n",
    "VOCAB_SIZE = len(vocab)\n",
    "model = Seq2SeqCaptioner(VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable    = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(model)\n",
    "print(f\"\\nâœ… Total parameters : {total_params:,}\")\n",
    "print(f\"   Trainable        : {trainable:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54936bb6",
   "metadata": {},
   "source": [
    "## ğŸš€ Part 5: Training Loop with Validation\n",
    "\n",
    "- **Loss:** CrossEntropyLoss (`ignore_index = <pad>`)\n",
    "- **Optimizer:** Adam (lr = 1e-3)\n",
    "- **Scheduler:** ReduceLROnPlateau\n",
    "- **Early Stopping:** patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Part 5 â€“ Training & Validation\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "PAD_IDX   = vocab.word2idx[Vocabulary.PAD]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                        factor=0.5, patience=3)\n",
    "\n",
    "NUM_EPOCHS     = 25\n",
    "PATIENCE       = 5\n",
    "best_val_loss  = float('inf')\n",
    "patience_count = 0\n",
    "train_losses   = []\n",
    "val_losses     = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # â”€â”€ TRAIN â”€â”€\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for feats, caps, lengths in tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS} [Train]\", leave=False):\n",
    "        feats, caps = feats.to(device), caps.to(device)\n",
    "        logits = model(feats, caps)                       # (B, T-1, V)\n",
    "        targets = caps[:, 1:]                              # (B, T-1)\n",
    "        loss = criterion(logits.reshape(-1, VOCAB_SIZE), targets.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_train = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_train)\n",
    "\n",
    "    # â”€â”€ VALIDATE â”€â”€\n",
    "    model.eval()\n",
    "    epoch_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for feats, caps, lengths in tqdm(val_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS} [Val]\", leave=False):\n",
    "            feats, caps = feats.to(device), caps.to(device)\n",
    "            logits = model(feats, caps)\n",
    "            targets = caps[:, 1:]\n",
    "            loss = criterion(logits.reshape(-1, VOCAB_SIZE), targets.reshape(-1))\n",
    "            epoch_val += loss.item()\n",
    "\n",
    "    avg_val = epoch_val / len(val_loader)\n",
    "    val_losses.append(avg_val)\n",
    "    scheduler.step(avg_val)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}  |  Train Loss: {avg_train:.4f}  |  Val Loss: {avg_val:.4f}  |  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # â”€â”€ Checkpoint & Early Stop â”€â”€\n",
    "    if avg_val < best_val_loss:\n",
    "        best_val_loss = avg_val\n",
    "        patience_count = 0\n",
    "        torch.save(model.state_dict(), MODEL_FILE)\n",
    "        print(f\"   âœ… Best model saved (val loss {best_val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_count += 1\n",
    "        if patience_count >= PATIENCE:\n",
    "            print(f\"   â¹ Early stopping after {epoch} epochs\")\n",
    "            break\n",
    "\n",
    "# Reload best weights\n",
    "model.load_state_dict(torch.load(MODEL_FILE, map_location=device))\n",
    "print(f\"\\nâœ… Training complete. Best Val Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ee4bf1",
   "metadata": {},
   "source": [
    "## ğŸ“‰ Part 6: Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Part 6 â€“ Plot Training & Validation Loss\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "plt.plot(epochs_range, train_losses, 'b-o', label='Training Loss')\n",
    "plt.plot(epochs_range, val_losses,   'r-o', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('Training & Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(BASE_DIR, 'loss_curve.png'), dpi=150)\n",
    "plt.show()\n",
    "print(\"âœ… Loss curve saved to /content/loss_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b8398",
   "metadata": {},
   "source": [
    "## ğŸ” Part 7: Inference â€“ Greedy Search & Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf4b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Part 7 â€“ Greedy Search & Beam Search\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_search(model, feature, vocab, max_len=MAX_LEN):\n",
    "    \"\"\"Generate a caption using greedy (argmax) decoding.\"\"\"\n",
    "    model.eval()\n",
    "    feature = feature.unsqueeze(0).to(device)                    # (1, 2048)\n",
    "    enc_out = model.encoder(feature)                              # (1, hidden)\n",
    "    h = enc_out.unsqueeze(0).repeat(model.num_layers, 1, 1)\n",
    "    c = torch.zeros_like(h)\n",
    "\n",
    "    word_idx = vocab.word2idx[Vocabulary.START]\n",
    "    words = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        inp = torch.tensor([[word_idx]], device=device)           # (1, 1)\n",
    "        logits, h, c = model.decoder(inp, h, c)                  # (1,1,V)\n",
    "        word_idx = logits.argmax(dim=-1).item()\n",
    "        word = vocab.idx2word[word_idx]\n",
    "        if word == Vocabulary.END:\n",
    "            break\n",
    "        words.append(word)\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def beam_search(model, feature, vocab, beam_width=3, max_len=MAX_LEN):\n",
    "    \"\"\"Generate a caption using beam search.\"\"\"\n",
    "    model.eval()\n",
    "    feature = feature.unsqueeze(0).to(device)\n",
    "    enc_out = model.encoder(feature)\n",
    "    h0 = enc_out.unsqueeze(0).repeat(model.num_layers, 1, 1)\n",
    "    c0 = torch.zeros_like(h0)\n",
    "\n",
    "    start_idx = vocab.word2idx[Vocabulary.START]\n",
    "    end_idx   = vocab.word2idx[Vocabulary.END]\n",
    "\n",
    "    # Each beam: (log_prob, token_list, h, c)\n",
    "    beams = [(0.0, [start_idx], h0, c0)]\n",
    "    completed = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        for log_prob, seq, h, c in beams:\n",
    "            inp = torch.tensor([[seq[-1]]], device=device)\n",
    "            logits, h_new, c_new = model.decoder(inp, h, c)       # (1,1,V)\n",
    "            log_probs = F.log_softmax(logits.squeeze(0).squeeze(0), dim=-1)  # (V,)\n",
    "            topk_probs, topk_idx = log_probs.topk(beam_width)\n",
    "\n",
    "            for k in range(beam_width):\n",
    "                token = topk_idx[k].item()\n",
    "                new_lp = log_prob + topk_probs[k].item()\n",
    "                new_seq = seq + [token]\n",
    "                if token == end_idx:\n",
    "                    completed.append((new_lp / len(new_seq), new_seq))\n",
    "                else:\n",
    "                    new_beams.append((new_lp, new_seq, h_new, c_new))\n",
    "\n",
    "        # Keep top beam_width beams\n",
    "        new_beams.sort(key=lambda x: x[0], reverse=True)\n",
    "        beams = new_beams[:beam_width]\n",
    "\n",
    "        if len(beams) == 0:\n",
    "            break\n",
    "\n",
    "    if completed:\n",
    "        completed.sort(key=lambda x: x[0], reverse=True)\n",
    "        best_seq = completed[0][1]\n",
    "    else:\n",
    "        best_seq = beams[0][1]\n",
    "\n",
    "    words = [vocab.idx2word[idx] for idx in best_seq\n",
    "             if vocab.idx2word[idx] not in (Vocabulary.START, Vocabulary.END, Vocabulary.PAD)]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "# â”€â”€ Quick test on one image â”€â”€\n",
    "sample_img = list(test_imgs)[0]\n",
    "sample_feat = torch.tensor(features_dict[sample_img], dtype=torch.float32)\n",
    "print(f\"Image: {sample_img}\")\n",
    "print(f\"  Greedy : {greedy_search(model, sample_feat, vocab)}\")\n",
    "print(f\"  Beam(3): {beam_search(model, sample_feat, vocab, beam_width=3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd81ef4",
   "metadata": {},
   "source": [
    "## ğŸ–¼ï¸ Part 8: Display 5 Random Test Images with Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Part 8 â€“ Show 5 Random Test Images with Captions\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "sample_test_imgs = random.sample(sorted(test_imgs), 5)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(25, 6))\n",
    "for ax, img_name in zip(axes, sample_test_imgs):\n",
    "    img_path = os.path.join(IMAGE_DIR, img_name)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Ground truth (first caption)\n",
    "    gt = captions_df[captions_df['image'] == img_name]['caption_clean'].values[0]\n",
    "\n",
    "    feat = torch.tensor(features_dict[img_name], dtype=torch.float32)\n",
    "    greedy_cap = greedy_search(model, feat, vocab)\n",
    "    beam_cap   = beam_search(model, feat, vocab, beam_width=3)\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"GT: {gt[:60]}...\\n\"\n",
    "        f\"Greedy: {greedy_cap}\\n\"\n",
    "        f\"Beam: {beam_cap}\",\n",
    "        fontsize=8, wrap=True\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Test Set â€“ Ground Truth vs Generated Captions\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(BASE_DIR, 'sample_captions.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e847fc",
   "metadata": {},
   "source": [
    "## ğŸ“Š Part 9: Quantitative Evaluation (BLEU-4, Precision, Recall, F1, METEOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1116622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Part 9 â€“ Quantitative Evaluation\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Group ground-truth captions per image for test set\n",
    "test_gt = {}\n",
    "for _, row in test_df.iterrows():\n",
    "    img = row['image']\n",
    "    cap = row['caption_clean']\n",
    "    test_gt.setdefault(img, []).append(cap.split())\n",
    "\n",
    "# Generate one prediction per test image\n",
    "references = []\n",
    "hypotheses = []\n",
    "meteor_scores = []\n",
    "\n",
    "print(\"Generating captions for test set...\")\n",
    "for img_name in tqdm(sorted(test_gt.keys())):\n",
    "    feat = torch.tensor(features_dict[img_name], dtype=torch.float32)\n",
    "    pred = beam_search(model, feat, vocab, beam_width=3)\n",
    "    pred_tokens = pred.split()\n",
    "\n",
    "    refs = test_gt[img_name]   # list of reference token-lists\n",
    "    references.append(refs)\n",
    "    hypotheses.append(pred_tokens)\n",
    "\n",
    "    # METEOR (per-sentence, then averaged later)\n",
    "    m = nltk_meteor(\n",
    "        [word_tokenize(' '.join(r)) for r in refs],\n",
    "        word_tokenize(pred)\n",
    "    )\n",
    "    meteor_scores.append(m)\n",
    "\n",
    "# â”€â”€ BLEU-4 â”€â”€\n",
    "smoother = SmoothingFunction().method1\n",
    "bleu4 = corpus_bleu(references, hypotheses,\n",
    "                    weights=(0.25, 0.25, 0.25, 0.25),\n",
    "                    smoothing_function=smoother)\n",
    "\n",
    "# â”€â”€ Token-level Precision / Recall / F1 â”€â”€\n",
    "total_tp, total_fp, total_fn = 0, 0, 0\n",
    "for refs, hyp in zip(references, hypotheses):\n",
    "    ref_tokens = set()\n",
    "    for r in refs:\n",
    "        ref_tokens.update(r)\n",
    "    hyp_set = set(hyp)\n",
    "    tp = len(hyp_set & ref_tokens)\n",
    "    fp = len(hyp_set - ref_tokens)\n",
    "    fn = len(ref_tokens - hyp_set)\n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "    total_fn += fn\n",
    "\n",
    "precision = total_tp / (total_tp + total_fp + 1e-8)\n",
    "recall    = total_tp / (total_tp + total_fn + 1e-8)\n",
    "f1        = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "avg_meteor = np.mean(meteor_scores)\n",
    "\n",
    "# â”€â”€ Print Results â”€â”€\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"     QUANTITATIVE EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  BLEU-4     : {bleu4:.4f}\")\n",
    "print(f\"  Precision  : {precision:.4f}\")\n",
    "print(f\"  Recall     : {recall:.4f}\")\n",
    "print(f\"  F1-Score   : {f1:.4f}\")\n",
    "print(f\"  METEOR     : {avg_meteor:.4f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "eval_results = {\n",
    "    \"BLEU-4\": round(bleu4, 4),\n",
    "    \"Precision\": round(precision, 4),\n",
    "    \"Recall\": round(recall, 4),\n",
    "    \"F1\": round(f1, 4),\n",
    "    \"METEOR\": round(avg_meteor, 4),\n",
    "}\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf1d82f",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Part 10: Save Model Artifacts to `/content/deploy/`\n",
    "\n",
    "All files needed for the Gradio Hugging Face Space will be saved here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e19af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Part 10 â€“ Save Deployment Artifacts to /content/deploy/\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import shutil\n",
    "\n",
    "os.makedirs(DEPLOY_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Model weights\n",
    "deploy_model_path = os.path.join(DEPLOY_DIR, \"best_model.pth\")\n",
    "shutil.copy(MODEL_FILE, deploy_model_path)\n",
    "\n",
    "# 2) Vocabulary\n",
    "deploy_vocab_path = os.path.join(DEPLOY_DIR, \"vocab.pkl\")\n",
    "shutil.copy(VOCAB_FILE, deploy_vocab_path)\n",
    "\n",
    "# 3) Config JSON (hyper-parameters needed at inference)\n",
    "config = {\n",
    "    \"embed_size\":   EMBED_SIZE,\n",
    "    \"hidden_size\":  HIDDEN_SIZE,\n",
    "    \"num_layers\":   NUM_LAYERS,\n",
    "    \"vocab_size\":   VOCAB_SIZE,\n",
    "    \"max_len\":      MAX_LEN,\n",
    "    \"dropout\":      DROPOUT,\n",
    "    \"beam_width\":   3,\n",
    "}\n",
    "config_path = os.path.join(DEPLOY_DIR, \"config.json\")\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "# Verify\n",
    "print(\"âœ… Deployment artifacts saved to /content/deploy/\")\n",
    "for fname in os.listdir(DEPLOY_DIR):\n",
    "    fpath = os.path.join(DEPLOY_DIR, fname)\n",
    "    size_kb = os.path.getsize(fpath) / 1024\n",
    "    print(f\"   {fname:30s}  {size_kb:>8.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9002f8a",
   "metadata": {},
   "source": [
    "## ğŸŒ Part 11: Generate Gradio `app.py` for Hugging Face Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b6a152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Part 11 â€“ Write app.py for Gradio / HuggingFace Space\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "app_code = r'''#!/usr/bin/env python3\n",
    "\"\"\"Neural Storyteller â€“ Gradio App for Hugging Face Spaces.\"\"\"\n",
    "\n",
    "import os, json, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "\n",
    "# â”€â”€ Device â”€â”€\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# â”€â”€ Load config â”€â”€\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "EMBED_SIZE  = cfg[\"embed_size\"]\n",
    "HIDDEN_SIZE = cfg[\"hidden_size\"]\n",
    "NUM_LAYERS  = cfg[\"num_layers\"]\n",
    "VOCAB_SIZE  = cfg[\"vocab_size\"]\n",
    "MAX_LEN     = cfg[\"max_len\"]\n",
    "DROPOUT     = cfg[\"dropout\"]\n",
    "BEAM_WIDTH  = cfg[\"beam_width\"]\n",
    "\n",
    "# â”€â”€ Load vocabulary â”€â”€\n",
    "with open(\"vocab.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "# â”€â”€ Model Definitions (must match training code) â”€â”€\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, feature_dim=2048, hidden_size=HIDDEN_SIZE):\n",
    "        super().__init__()\n",
    "        self.fc   = nn.Linear(feature_dim, hidden_size)\n",
    "        self.bn   = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(DROPOUT)\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.drop(self.relu(self.bn(self.fc(features))))\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=EMBED_SIZE,\n",
    "                 hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS):\n",
    "        super().__init__()\n",
    "        self.embed  = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.lstm   = nn.LSTM(embed_size, hidden_size,\n",
    "                              num_layers=num_layers, batch_first=True,\n",
    "                              dropout=DROPOUT if num_layers > 1 else 0)\n",
    "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.drop   = nn.Dropout(DROPOUT)\n",
    "\n",
    "    def forward(self, captions, hidden, cell):\n",
    "        embeds = self.drop(self.embed(captions))\n",
    "        output, (hidden, cell) = self.lstm(embeds, (hidden, cell))\n",
    "        logits = self.fc_out(output)\n",
    "        return logits, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2SeqCaptioner(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=EMBED_SIZE,\n",
    "                 hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS):\n",
    "        super().__init__()\n",
    "        self.encoder    = Encoder(2048, hidden_size)\n",
    "        self.decoder    = Decoder(vocab_size, embed_size, hidden_size, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        enc_out = self.encoder(features)\n",
    "        h0 = enc_out.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        c0 = torch.zeros_like(h0)\n",
    "        logits, _, _ = self.decoder(captions[:, :-1], h0, c0)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# â”€â”€ Load trained weights â”€â”€\n",
    "caption_model = Seq2SeqCaptioner(VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(device)\n",
    "caption_model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
    "caption_model.eval()\n",
    "\n",
    "# â”€â”€ ResNet50 feature extractor â”€â”€\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "# â”€â”€ Beam Search â”€â”€\n",
    "@torch.no_grad()\n",
    "def beam_search_inference(feature, beam_width=BEAM_WIDTH):\n",
    "    feature = feature.unsqueeze(0).to(device)\n",
    "    enc_out = caption_model.encoder(feature)\n",
    "    h0 = enc_out.unsqueeze(0).repeat(caption_model.num_layers, 1, 1)\n",
    "    c0 = torch.zeros_like(h0)\n",
    "\n",
    "    start_idx = vocab.word2idx[vocab.START]\n",
    "    end_idx   = vocab.word2idx[vocab.END]\n",
    "\n",
    "    beams = [(0.0, [start_idx], h0, c0)]\n",
    "    completed = []\n",
    "\n",
    "    for _ in range(MAX_LEN):\n",
    "        new_beams = []\n",
    "        for log_prob, seq, h, c in beams:\n",
    "            inp = torch.tensor([[seq[-1]]], device=device)\n",
    "            logits, h_new, c_new = caption_model.decoder(inp, h, c)\n",
    "            log_probs = F.log_softmax(logits.squeeze(0).squeeze(0), dim=-1)\n",
    "            topk_probs, topk_idx = log_probs.topk(beam_width)\n",
    "\n",
    "            for k in range(beam_width):\n",
    "                token = topk_idx[k].item()\n",
    "                new_lp = log_prob + topk_probs[k].item()\n",
    "                new_seq = seq + [token]\n",
    "                if token == end_idx:\n",
    "                    completed.append((new_lp / len(new_seq), new_seq))\n",
    "                else:\n",
    "                    new_beams.append((new_lp, new_seq, h_new, c_new))\n",
    "\n",
    "        new_beams.sort(key=lambda x: x[0], reverse=True)\n",
    "        beams = new_beams[:beam_width]\n",
    "        if not beams:\n",
    "            break\n",
    "\n",
    "    if completed:\n",
    "        completed.sort(key=lambda x: x[0], reverse=True)\n",
    "        best_seq = completed[0][1]\n",
    "    else:\n",
    "        best_seq = beams[0][1] if beams else [start_idx]\n",
    "\n",
    "    words = [vocab.idx2word[idx] for idx in best_seq\n",
    "             if vocab.idx2word[idx] not in (vocab.START, vocab.END, vocab.PAD)]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# â”€â”€ Prediction function for Gradio â”€â”€\n",
    "def predict(image):\n",
    "    \"\"\"Take a PIL image â†’ return generated caption string.\"\"\"\n",
    "    if image is None:\n",
    "        return \"Please upload an image.\"\n",
    "    image = image.convert(\"RGB\")\n",
    "    img_tensor = img_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feature = resnet(img_tensor).view(1, -1).squeeze(0)\n",
    "\n",
    "    caption = beam_search_inference(feature)\n",
    "    return caption\n",
    "\n",
    "\n",
    "# â”€â”€ Gradio Interface â”€â”€\n",
    "demo = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Upload an Image\"),\n",
    "    outputs=gr.Textbox(label=\"Generated Caption\"),\n",
    "    title=\"ğŸ§  Neural Storyteller â€“ Image Captioning\",\n",
    "    description=(\n",
    "        \"Upload any image and this Seq2Seq model (ResNet50 encoder + LSTM decoder) \"\n",
    "        \"trained on Flickr30k will generate a natural language caption using beam search.\"\n",
    "    ),\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "'''\n",
    "\n",
    "app_path = os.path.join(DEPLOY_DIR, \"app.py\")\n",
    "with open(app_path, 'w') as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(f\"âœ… app.py written to {app_path}\")\n",
    "print(f\"   Size: {os.path.getsize(app_path) / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a75ef",
   "metadata": {},
   "source": [
    "## ğŸ“„ Part 12: Generate `requirements.txt` & `README.md` for HF Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4e2b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Part 12 â€“ requirements.txt & README.md\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# â”€â”€ requirements.txt â”€â”€\n",
    "requirements = \"\"\"torch\n",
    "torchvision\n",
    "gradio\n",
    "Pillow\n",
    "numpy\n",
    "\"\"\"\n",
    "req_path = os.path.join(DEPLOY_DIR, \"requirements.txt\")\n",
    "with open(req_path, 'w') as f:\n",
    "    f.write(requirements.strip() + '\\n')\n",
    "print(f\"âœ… requirements.txt â†’ {req_path}\")\n",
    "\n",
    "# â”€â”€ README.md (HuggingFace Space metadata) â”€â”€\n",
    "readme = \"\"\"---\n",
    "title: Neural Storyteller Image Captioning\n",
    "emoji: ğŸ§ \n",
    "colorFrom: blue\n",
    "colorTo: purple\n",
    "sdk: gradio\n",
    "sdk_version: \"4.44.0\"\n",
    "app_file: app.py\n",
    "pinned: false\n",
    "---\n",
    "\n",
    "# ğŸ§  Neural Storyteller â€“ Image Captioning with Seq2Seq\n",
    "\n",
    "Upload any image and this model will generate a natural language caption!\n",
    "\n",
    "## Architecture\n",
    "- **Encoder:** Pre-trained ResNet50 (2048-d features) â†’ Linear projection (512-d)\n",
    "- **Decoder:** LSTM with learned word embeddings\n",
    "- **Inference:** Beam Search (width = 3)\n",
    "\n",
    "## Dataset\n",
    "Trained on **Flickr30k** (31,783 images, 5 captions each).\n",
    "\n",
    "## Metrics\n",
    "| Metric | Score |\n",
    "|--------|-------|\n",
    "| BLEU-4 | See notebook |\n",
    "| METEOR | See notebook |\n",
    "| Token F1 | See notebook |\n",
    "\n",
    "## Files\n",
    "- `app.py` â€“ Gradio application\n",
    "- `best_model.pth` â€“ Trained Seq2Seq weights\n",
    "- `vocab.pkl` â€“ Vocabulary (word â†” index)\n",
    "- `config.json` â€“ Model hyper-parameters\n",
    "\"\"\"\n",
    "readme_path = os.path.join(DEPLOY_DIR, \"README.md\")\n",
    "with open(readme_path, 'w') as f:\n",
    "    f.write(readme)\n",
    "print(f\"âœ… README.md â†’ {readme_path}\")\n",
    "\n",
    "# â”€â”€ Summary â”€â”€\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  ğŸ“¦  /content/deploy/  â€“  READY FOR HUGGING FACE SPACE\")\n",
    "print(\"=\" * 60)\n",
    "for fname in sorted(os.listdir(DEPLOY_DIR)):\n",
    "    fpath = os.path.join(DEPLOY_DIR, fname)\n",
    "    size_kb = os.path.getsize(fpath) / 1024\n",
    "    print(f\"   {fname:30s}  {size_kb:>10.1f} KB\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nğŸ“‹ Upload instructions:\")\n",
    "print(\"   1. Go to https://huggingface.co/new-space\")\n",
    "print(\"   2. Choose SDK â†’ Gradio\")\n",
    "print(\"   3. Upload ALL files from /content/deploy/\")\n",
    "print(\"   4. Or use git:\")\n",
    "print(\"      git clone https://huggingface.co/spaces/YOUR_USER/YOUR_SPACE\")\n",
    "print(\"      cp /content/deploy/* YOUR_SPACE/\")\n",
    "print(\"      cd YOUR_SPACE && git add . && git commit -m 'init' && git push\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05e010d",
   "metadata": {},
   "source": [
    "## ğŸ§ª Part 13: Test Gradio App Locally in Colab\n",
    "\n",
    "Run the cell below to launch the Gradio interface with a **public shareable link**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b8064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Part 13 â€“ Launch Gradio App in Colab for Live Testing\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, DEPLOY_DIR)\n",
    "\n",
    "# Quick programmatic test on a sample test image\n",
    "sample_img_name = random.choice(sorted(test_imgs))\n",
    "sample_image = Image.open(os.path.join(IMAGE_DIR, sample_img_name)).convert('RGB')\n",
    "print(f\"Testing on: {sample_img_name}\")\n",
    "\n",
    "# Extract feature and generate caption using existing model (already loaded)\n",
    "test_tensor = img_transform(sample_image).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    # Use the resnet that's already loaded from Part 1 (remove DataParallel wrapper)\n",
    "    resnet_single = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    resnet_single = nn.Sequential(*list(resnet_single.children())[:-1]).to(device)\n",
    "    resnet_single.eval()\n",
    "    test_feat = resnet_single(test_tensor).view(1, -1).squeeze(0)\n",
    "\n",
    "caption = beam_search(model, test_feat, vocab, beam_width=3)\n",
    "gt_caps = captions_df[captions_df['image'] == sample_img_name]['caption_clean'].values\n",
    "print(f\"  Ground Truth : {gt_caps[0]}\")\n",
    "print(f\"  Generated    : {caption}\")\n",
    "\n",
    "# â”€â”€ Launch Gradio inline â”€â”€\n",
    "print(\"\\nğŸš€ Launching Gradio interface...\")\n",
    "\n",
    "# Define predict function using loaded model\n",
    "def colab_predict(image):\n",
    "    if image is None:\n",
    "        return \"Please upload an image.\"\n",
    "    image = image.convert(\"RGB\")\n",
    "    t = img_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        feat = resnet_single(t).view(1, -1).squeeze(0)\n",
    "    return beam_search(model, feat, vocab, beam_width=3)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=colab_predict,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Upload an Image\"),\n",
    "    outputs=gr.Textbox(label=\"Generated Caption\"),\n",
    "    title=\"ğŸ§  Neural Storyteller â€“ Image Captioning\",\n",
    "    description=(\n",
    "        \"Upload any image and this Seq2Seq model (ResNet50 encoder + LSTM decoder) \"\n",
    "        \"trained on Flickr30k will generate a natural language caption using beam search.\"\n",
    "    ),\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "\n",
    "demo.launch(share=True, debug=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
